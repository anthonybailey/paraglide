---
title: Umfragen & Studien
description: Wie sehr machen sich Laien und Experten Sorgen um die Risiken und die Regulierung von KI?
---

## Expertenmeinung zu katastrophalen Risiken {#expert-opinion-on-catastrophic-risks}

- **[KI-Forscher, AIImpacts 2022](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/)**: schätzen die Wahrscheinlichkeit von "sehr schlechten Ergebnissen (wie dem Aussterben der Menschheit)" auf 14%, mit einem Median von 5%. 82% halten das Kontrollproblem für wichtig.
- **[KI-Forscher, AIImpacts 2023](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai)**: Die durchschnittliche Wahrscheinlichkeit für ein katastrophales Ereignis (p(doom)) liegt zwischen 14 und 19,4%, je nach Formulierung der Frage. 86% halten das Kontrollproblem für wichtig.
- **[KI-Ingenieure / Startup-Gründer, State of AI Engineering](https://elemental-croissant-32a.notion.site/State-of-AI-Engineering-2023-20c09dc1767f45988ee1f479b4a84135#694f89e86f9148cb855220ec05e9c631)**: Über 60% haben eine [p(doom)](/pdoom) > 25%. Nur 12% haben eine p(doom) = 0.
- **[KI-Sicherheitsforscher, AlignmentForum](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results)**: Die Befragten schätzten die Wahrscheinlichkeit für ein existenzielles Risiko aufgrund mangelnder technischer Forschung auf 20% und aufgrund eines Versagens von KI-Systemen, die Absichten der Menschen zu erfüllen, auf 30%, mit großer Variation (z.B. gibt es Datenpunkte bei ~1% und ~99%).

## Öffentliche Meinung zu katastrophalen Risiken {#public-opinion-on-catastrophic-risks}

- **[Bürger im Vereinigten Königreich, PublicFirst](https://publicfirst.co.uk/ai/)**: glauben, dass es eine 9%ige Wahrscheinlichkeit gibt, dass die Menschheit aufgrund von KI aussterben wird. Etwa 50% sagen, sie seien sehr oder einigermaßen besorgt darüber.
- **[Bürger in Deutschland, Kira](https://www.zeit.de/digital/2023-04/ki-risiken-angst-umfrage-forschung-kira)**: Nur 14% glauben, dass KI einen positiven Einfluss auf die Welt haben wird, 40% sind unentschieden, 40% negativ.
- **[Bürger in den USA, RethinkPriorities](https://rethinkpriorities.org/publications/us-public-perception-of-cais-statement-and-the-risk-of-extinction)**: stimmen der Aussage zu (59%) und unterstützen sie (58%), dass KI ein existenzielles Risiko darstellt. Die Ablehnung (26%) und Opposition (22%) waren relativ niedrig, und ein erheblicher Anteil der Befragten blieb neutral (12% bzw. 18% für die Zustimmung und Unterstützung).
- **[Bürger in Australien, Ready Research](https://theconversation.com/80-of-australians-think-ai-risk-is-a-global-priority-the-government-needs-to-step-up-225175)**: 80% glauben, dass das KI-Risiko eine globale Priorität ist, 64% wollen, dass die Regierung sich auf katastrophale Ergebnisse konzentriert (im Vergleich zu nur 25% auf Arbeitsplatzverlust oder 5% auf Voreingenommenheit).

## Öffentliche Meinung zu Regulierung und Governance {#public-opinion-on-regulations--governance}

- [**Bürger in den USA, RethinkPriorities**](https://forum.effectivealtruism.org/posts/ConFiY9cRmg37fs2p/us-public-opinion-of-ai-policy-and-risk): 50% unterstützen eine Pause, 25% lehnen eine Pause ab.
- [**Bürger in den USA, YouGov**](https://www.vox.com/future-perfect/2023/8/18/23836362/ai-slow-down-poll-regulation): 72% wollen, dass KI langsamer entwickelt wird, 8% wollen eine Beschleunigung. 83% der Wähler glauben, dass KI versehentlich ein katastrophales Ereignis verursachen könnte.
- [**Bürger in den USA, YouGov**](https://theaipi.org/poll-shows-voters-oppose-open-sourcing-ai-models-support-regulatory-representation-on-boards-and-say-ai-risks-outweigh-benefits-2/): 73% glauben, dass KI-Unternehmen für Schäden durch die von ihnen entwickelte Technologie haftbar gemacht werden sollten, 67% denken, dass die Leistungsfähigkeit von KI-Modellen eingeschränkt werden sollte, und 65% glauben, dass es wichtiger ist, KI aus den Händen von böswilligen Akteuren fernzuhalten, als die Vorteile von KI allen zugänglich zu machen.
- [**Bürger in den USA, AIPI**](https://www.politico.com/newsletters/digital-future-daily/2023/11/29/exclusive-what-people-actually-think-about-ai-00129147): 49:20 unterstützen "einen internationalen Vertrag, um jede 'intelligenter-als-menschliche' künstliche Intelligenz (KI) zu verbieten?", 70:14 unterstützen "die Verhinderung, dass KI schnell übermenschliche Fähigkeiten erreicht".
- [**US-amerikanische Informatik-Professoren, Axios Generation Lab**](https://www.axios.com/2023/09/05/ai-regulations-expert-survey): Etwa 1 von 5 sagte voraus, dass KI "definitiv" unter menschlicher Kontrolle bleiben wird. Der Rest war zwischen denen, die sagten, KI werde "wahrscheinlich" oder "definitiv" außer Kontrolle geraten, und denen, die sagten, "wahrscheinlich nicht", aufgeteilt.
  Nur 1 von 6 sagte, KI sollte nicht oder könne nicht reguliert werden. Nur eine Handvoll vertraut dem privaten Sektor, sich selbst zu regulieren.
- [**Bürger in den USA, Sentience Institute**](https://www.sentienceinstitute.org/aims-survey-supplement-2023): Es gab breite Unterstützung für Schritte, die unternommen werden könnten, um die Entwicklung zu verlangsamen. Die Menschen unterstützten öffentliche Kampagnen, um die KI-Entwicklung zu verlangsamen (71,3%), staatliche Regulierung, die die Entwicklung verlangsamt (71,0%), und eine sechsmonatige Pause bei bestimmten Arten von KI-Entwicklungen (69,1%). Die Unterstützung für ein Verbot von künstlicher allgemeiner Intelligenz (AGI), die intelligenter ist als Menschen, lag bei 62,9%.
- [**Bürger im Vereinigten Königreich, YouGov**](https://inews.co.uk/news/politics/voters-deepfakes-ban-ai-intelligent-humans-2708693): 74% glauben, dass die Regierung die Schaffung von übermenschlicher KI verhindern sollte. Über 60% unterstützen einen Vertrag mit einem globalen Verbot von Superintelligenz.
- [**Bürger im Vereinigten Königreich, AISCC**](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/): 83% der Befragten sagten, dass die Regierung von KI-Unternehmen verlangen sollte, dass sie beweisen, dass ihre KI-Modelle sicher sind, bevor sie sie veröffentlichen.
- [**Bürger in den Niederlanden, den USA und dem Vereinigten Königreich, Existential Risk Observatory**](https://www.existentialriskobservatory.org/papers_and_reports/Trends%20in%20Public%20Attitude%20Towards%20Existential%20Risk%20And%20Artificial%20Intelligence.pdf): Das öffentliche Bewusstsein für existenzielle Risiken wuchs in den USA von 7% auf 15% und in den Niederlanden und dem Vereinigten Königreich auf 19%. Die Unterstützung für eine staatlich angeordnete KI-Pause stieg in den USA von 56% auf 66%.

## [Zeitpläne](/timelines) {#timelines}

- [**Metaculus Weak AGI**](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) vor 2026: 25% Chance, AGI bis 2027: 50% Chance (aktualisiert am 2024-11-05).
- [**Metaculus full AGI**](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) vor 2028: 25% Chance, vollständige AGI bis 2032: 50% Chance (aktualisiert am 2024-11-05).