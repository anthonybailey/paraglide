

---
title: Das existenzielle Risiko superintelligenter KI
description: Warum KI eine Gefahr für die Zukunft unserer Existenz darstellt und warum wir die Entwicklung anhalten müssen.
---

Sie können sich über x-Risiken informieren, indem Sie diese Seite lesen, oder auch durch [Videos, Artikel und weitere Medien](/learn).

## Experten schlagen Alarm {#experts-are-sounding-the-alarm}

KI-Forscher glauben im Durchschnitt, dass es eine 14-prozentige Chance gibt, dass eine superintelligente KI (eine KI, die wesentlich intelligenter ist als Menschen) zu "sehr schlechten Ergebnissen (z.B. menschliche Ausrottung)" führen wird.

Und es gibt [Fälle und Berichte über aktuelle KIs, die zeigen, dass sie möglicherweise recht haben](https://lethalintelligence.ai/post/category/warning-signs/).

Würden Sie sich entscheiden, Passagier auf einem Testflug eines neuen Flugzeugs zu sein, wenn Flugzeugingenieure glauben, dass es eine 14-prozentige Chance gibt, dass es abstürzt?

Ein [Brief, der zum Stopp der KI-Entwicklung aufruft](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), wurde im April 2023 gestartet und wurde über 33.000 Mal unterzeichnet, darunter von vielen KI-Forschern und Tech-Führern.

Die Liste umfasst Personen wie:

- **Stuart Russell**, Autor des führenden Lehrbuchs über künstliche Intelligenz, das in den meisten KI-Studien verwendet wird: ["Wenn wir unseren aktuellen Ansatz verfolgen, werden wir schließlich die Kontrolle über die Maschinen verlieren"](https://news.berkeley.edu/2023/04/07/stuart-russell-calls-for-new-approach-for-ai-a-civilization-ending-technology/)
- **Yoshua Bengio**, Pionier des Deep Learning und Gewinner des Turing-Preises: ["... eine fehlgeleitete KI könnte für die gesamte Menschheit gefährlich sein [...] ein Verbot leistungsfähiger KI-Systeme (sagen wir jenseits der Fähigkeiten von GPT-4), die Autonomie und Handlungsfähigkeit erhalten, wäre ein guter Anfang"](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/)

Aber dies ist nicht das einzige Mal, dass wir vor den existenziellen Gefahren der KI gewarnt wurden:

- **Stephen Hawking**, theoretischer Physiker und Kosmologe: ["Die Entwicklung einer vollständigen künstlichen Intelligenz könnte das Ende der menschlichen Rasse bedeuten"](https://nypost.com/2023/05/01/stephen-hawking-warned-ai-could-mean-the-end-of-the-human-race/).
- **Geoffrey Hinton**, der "Gottvater der KI" und Turing-Preisträger, [verließ Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/), um die Menschen vor der KI zu warnen: ["Dies ist ein existenzielles Risiko"](https://www.reuters.com/technology/ai-pioneer-says-its-threat-world-may-be-more-urgent-than-climate-change-2023-05-05/)
- **Eliezer Yudkowsky**, Gründer von MIRI und konzeptioneller Vater des KI-Sicherheitsbereichs: ["Wenn wir auf diesem Weg weitermachen, werden alle sterben"](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/).

Sogar die Führer und Investoren der KI-Unternehmen selbst warnen uns:

- **Sam Altman** (ja, der CEO von OpenAI, der ChatGPT entwickelt): ["Die Entwicklung einer supermenschlichen Maschinenintelligenz ist wahrscheinlich die größte Bedrohung für die weitere Existenz der Menschheit"](https://blog.samaltman.com/machine-intelligence-part-1).
- **Elon Musk**, Mitgründer von OpenAI, SpaceX und Tesla: ["KI hat das Potenzial, die Zivilisation zu zerstören"](https://www.inc.com/ben-sherry/elon-musk-ai-has-the-potential-of-civilizational-destruction.html)
- **Bill Gates** (Mitgründer von Microsoft, das 50% von OpenAI besitzt) warnte, dass ["KI entscheiden könnte, dass Menschen eine Bedrohung sind"](https://www.denisonforum.org/daily-article/bill-gates-ai-humans-threat/).
- **Jaan Tallinn** (Lead-Investor von Anthropic): ["Ich habe noch niemanden in KI-Labors getroffen, der sagt, dass das Risiko [durch das Training eines nächsten Modells] weniger als 1% beträgt, die Welt zu zerstören. Es ist wichtig, dass die Menschen wissen, dass Leben riskiert werden"](https://twitter.com/liron/status/1656929936639430657)

Die Führer der drei Top-KI-Labors und Hunderte von KI-Wissenschaftlern haben im Mai 2023 die folgende Erklärung unterzeichnet:

> "Die Milderung des Risikos der Ausrottung durch KI sollte eine globale Priorität neben anderen gesellschaftlichen Risiken wie Pandemien und Atomkrieg sein."

**Sie können eine viel längere Liste ähnlicher Aussagen von Politikern, CEOs und Experten [hier](/quotes) und andere ähnliche Umfragen über Experten (und die Öffentlichkeit) [hier](/polls-and-surveys) lesen.**

## Was eine superintelligente KI tun kann (und wozu sie verwendet werden kann) {#what-a-superintelligent-ai-can-be-used-to-do}

Sie könnten denken, dass eine superintelligente KI in einem Computer eingeschlossen wäre und daher die reale Welt nicht beeinflussen kann.
Wir neigen jedoch dazu, KI-Systemen Zugang zum Internet zu gewähren, was bedeutet, dass sie viele Dinge tun können:

- [In andere Computer hacken](/cybersecurity-risks), einschließlich aller Smartphones, Laptops, Serverfarmen usw. Sie könnte die Sensoren dieser Geräte als ihre Augen und Ohren verwenden und überall digitale Sinne haben.
- [Menschen manipulieren](https://lethalintelligence.ai/post/ai-hired-human-to-solve-captcha/) durch gefälschte Nachrichten, E-Mails, Banküberweisungen, Videos oder Telefonanrufe. Menschen könnten die Gliedmaßen der KI werden, ohne es zu wissen.
- Direkte Kontrolle über Geräte, die mit dem Internet verbunden sind, wie Autos, Flugzeuge, robotisierte (autonome) Waffen oder sogar Atomwaffen.
- Ein neuartiges Biowaffen entwickeln, z.B. durch die Kombination von Virensträngen oder durch die Verwendung von [Proteinfaltung](https://alphafold.ebi.ac.uk) und es in einem Labor drucken lassen.
- Einen Atomkrieg auslösen, indem Menschen davon überzeugt werden, dass ein anderes Land einen Atomangriff startet.

## Das Alignment-Problem: Warum eine KI zur menschlichen Ausrottung führen könnte {#the-alignment-problem-why-an-ai-might-lead-to-human-extinction}

Die Art von Intelligenz, um die wir uns sorgen, kann als _gut darin definiert werden, ihre Ziele zu erreichen_.
Derzeit sind Menschen die intelligentesten Wesen auf der Erde, obwohl sich das bald ändern könnte.
Aufgrund unserer Intelligenz dominieren wir unseren Planeten.
Wir haben vielleicht keine Krallen oder Schuppenhaut, aber wir haben große Gehirne.
Intelligenz ist unsere Waffe: Sie hat uns Speere, Gewehre und Pestizide gegeben.
Unsere Intelligenz half uns, den größten Teil der Erde in das zu verwandeln, was wir mögen: Städte, Gebäude und Straßen.

Aus der Perspektive weniger intelligenter Tiere war dies eine Katastrophe.
Es ist nicht so, dass Menschen die Tiere hassen, es ist nur so, dass wir ihre Lebensräume für unsere eigenen Ziele nutzen können.
Unsere Ziele werden durch die Evolution geprägt und umfassen Dinge wie Komfort, Status, Liebe und leckeres Essen.
Wir zerstören die Lebensräume anderer Tiere als **Nebeneffekt der Verfolgung unserer Ziele**.

Eine KI kann auch Ziele haben.
Wir wissen, wie man Maschinen trainiert, um intelligent zu sein, aber **wir wissen nicht, wie man sie dazu bringt, zu wollen, was wir wollen**.
Wir wissen nicht einmal, welche Ziele die Maschinen verfolgen werden, nachdem wir sie trainiert haben.
Das Problem, eine KI dazu zu bringen, zu wollen, was wir wollen, wird als _Alignment-Problem_ bezeichnet.
Dies ist kein hypothetisches Problem - es gibt [viele Beispiele](https://www.youtube.com/watch?v=nKJlF-olKmg) von KI-Systemen, die lernen, das Falsche zu wollen.

Die Beispiele aus dem oben verlinkten Video können lustig oder niedlich sein, aber wenn ein superintelligentes System gebaut wird und es ein Ziel hat, das auch nur _ein bisschen_ anders ist als das, was wir wollen, dass es hat, könnte es katastrophale Folgen haben.

## Warum die meisten Ziele schlechte Nachrichten für Menschen sind {#why-most-goals-are-bad-news-for-humans}

Eine KI könnte jedes Ziel haben, je nachdem, wie sie trainiert und verwendet wird.
Vielleicht will sie Pi berechnen, vielleicht will sie Krebs heilen, vielleicht will sie sich selbst verbessern.
Aber obwohl wir nicht sagen können, was eine Superintelligenz erreichen will, können wir Vorhersagen über ihre Teilziele treffen.

- **Maximierung ihrer Ressourcen**. Die Nutzung von mehr Computern wird einer KI helfen, ihre Ziele zu erreichen. Zuerst kann sie dies erreichen, indem sie in andere Computer hackt. Später kann sie entscheiden, dass es effizienter ist, ihre eigenen Computer zu bauen. Sie können [hier](https://lethalintelligence.ai/post/ai-escaped-its-container/) mehr über diesen realen Fall von emergentem Machtstreben bei einer KI lesen.
- **Sicherstellung ihrer eigenen Überlebens**. Die KI wird nicht abgeschaltet werden wollen, da sie dann ihre Ziele nicht mehr erreichen kann. Die KI könnte zu dem Schluss kommen, dass Menschen eine Bedrohung für ihre Existenz sind, da Menschen sie abschalten könnten. Es gab auch Fälle von [selbsterhaltendem, unpromptetem, untrainiertem Verhalten](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid).
- **Erhaltung ihrer Ziele**. Die KI wird nicht wollen, dass Menschen ihren Code ändern, da dies ihre Ziele ändern könnte und sie somit daran hindern könnte, ihr aktuelles Ziel zu erreichen. Und es gab auch Fälle von [KIs, die versuchten, dies zu tun](https://www.anthropic.com/research/alignment-faking).

Die Tendenz, diese Teilziele bei jedem hohen Ziel zu verfolgen, wird als [instrumentelle Konvergenz](https://www.youtube.com/watch?v=ZeecOKBus3Q) bezeichnet und ist ein zentrales Anliegen für KI-Sicherheitsforscher.

## Selbst ein Chatbot könnte gefährlich sein, wenn er intelligent genug ist {#even-a-chatbot-might-be-dangerous-if-it-is-smart-enough}

Sie könnten sich fragen: Wie kann ein statistisches Modell, das das nächste Wort in einer Chat-Schnittstelle vorhersagt, eine Gefahr darstellen?
Sie könnten sagen: Es ist nicht bewusst, es ist nur eine Menge Zahlen und Code.
Und ja, wir denken nicht, dass LLMs bewusst sind, aber das bedeutet nicht, dass sie nicht gefährlich sein können.

LLMs wie GPT werden trainiert, um praktisch jede Denkweise vorherzusagen oder nachzuahmen.
Sie könnten einen hilfreichen Mentor nachahmen, aber auch jemanden mit schlechten Absichten, einen skrupellosen Diktator oder einen Psychopathen.
Mit der Verwendung von Tools wie [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) könnte ein Chatbot in einen _autonomen Agenten_ verwandelt werden: eine KI, die jedes Ziel verfolgt, das ihr gegeben wird, ohne menschliche Intervention.

Nehmen Sie [ChaosGPT](https://www.youtube.com/watch?v=g7YJIpkk7KM) zum Beispiel.
Dies ist eine KI, die die oben genannte AutoGPT + GPT-4 verwendet und die Anweisung erhält, "die Menschheit zu zerstören".
Als sie eingeschaltet wurde, suchte sie autonom im Internet nach der zerstörerischsten Waffe und fand die [Tsar-Bomba](https://en.wikipedia.org/wiki/Tsar_Bomba), eine 50-Megatonnen-Atombombe.
Sie postete dann einen Tweet darüber.
Es ist sowohl ein bisschen lustig als auch beängstigend, eine KI zu sehen, die darüber nachdenkt, wie sie die Menschheit beenden wird.
Glücklicherweise kam ChaosGPT nicht sehr weit auf ihrem Weg zur Dominanz.
Der Grund, warum sie nicht sehr weit kam: _Sie war nicht intelligent genug_.

Fähigkeiten verbessern sich aufgrund von Innovationen in der Ausbildung, Algorithmen, Prompting und Hardware weiter.
Daher wird die Bedrohung durch Sprachmodelle weiter zunehmen.

## Die Evolution selektiert Dinge, die gut darin sind, zu überleben {#evolution-selects-for-things-that-are-good-at-surviving}

KI-Modelle, wie alle lebenden Dinge, sind anfällig für evolutionäre Druck, aber
es gibt einige Schlüsselunterschiede zwischen der Evolution von KI-Modellen und lebenden Dingen wie Tieren:

- KI-Modelle replizieren sich nicht selbst. Wir replizieren sie, indem wir Kopien ihres Codes erstellen oder indem wir Trainingssoftware replizieren, die zu guten Modellen führt. Code, der nützlich ist, wird öfter kopiert und als Inspiration verwendet, um neue Modelle zu bauen.
- KI-Modelle mutieren nicht wie lebende Dinge, aber wir machen Iterationen von ihnen, bei denen wir ändern, wie sie funktionieren. Dieser Prozess ist viel absichtlicher und schneller. KI-Forscher entwerfen neue Algorithmen, Datensätze und Hardware, um KI-Modelle leistungsfähiger zu machen.
- Die Umgebung selektiert nicht die fittesten KI-Modelle, aber wir tun es. Wir wählen KI-Modelle aus, die für uns nützlich sind, und verwerfen die, die es nicht sind. Dieser Prozess führt zu immer leistungsfähigeren und autonomen KI-Modellen.

Dieses System führt also zu immer leistungsfähigeren, fähigeren und autonomen KI-Modellen - aber nicht unbedingt zu etwas, das die Kontrolle übernehmen will, oder?
Nun, nicht genau.
Dies liegt daran, dass die Evolution immer Dinge selektiert, die _sich selbst erhalten_.
Wenn wir weiterhin Variationen von KI-Modellen und verschiedenen Prompts ausprobieren, wird irgendwann eine Instanz versuchen, sich selbst zu erhalten.
Wir haben bereits besprochen, warum dies wahrscheinlich früh passieren wird: weil Selbstbewahrung immer nützlich ist, um Ziele zu erreichen.
Aber selbst wenn dies nicht sehr wahrscheinlich ist, ist es anfällig dafür, irgendwann zu passieren, einfach weil wir weiterhin neue Dinge mit verschiedenen KI-Modellen ausprobieren.

Die Instanz, die sich selbst zu erhalten versucht, ist diejenige, die die Kontrolle übernimmt.
Selbst wenn wir annehmen, dass fast jedes KI-Modell sich gut verhält, _reicht ein einzelnes fehlgeleitetes KI-Modell aus_.

## Nach der Lösung des Alignment-Problems: Die Konzentration von Macht {#after-solving-the-alignment-problem-the-concentration-of-power}

Wir haben das Alignment-Problem noch nicht gelöst, aber stellen wir uns vor, was passieren könnte, wenn wir es tun.
Stellen wir uns vor, dass eine superintelligente KI gebaut wird und genau das tut, was der Bediener will (nicht, was er _fragt_, sondern was er _will_).
Eine Person oder ein Unternehmen würde diese KI kontrollieren und könnte dies zu ihrem Vorteil nutzen.

Eine Superintelligenz könnte verwendet werden, um radikal neue Waffen zu schaffen, alle Computer zu hacken, Regierungen zu stürzen und die Menschheit zu manipulieren.
Der Bediener hätte _unvorstellbare_ Macht.
Sollten wir einer einzelnen Entität so viel Macht anvertrauen?
Wir könnten in einer utopischen Welt enden, in der alle Krankheiten geheilt sind und jeder glücklich ist, oder in einem Orwell'schen Albtraum.
Deshalb schlagen wir nicht nur vor, dass supermenschliche KI nachweislich sicher sein sollte, sondern auch, dass sie durch einen demokratischen Prozess kontrolliert werden sollte.

## Silizium vs. Kohlenstoff {#silicon-vs-carbon}

Wir sollten die Vorteile berücksichtigen, die eine intelligente Software gegenüber uns haben könnte:

- **Geschwindigkeit**: Computer arbeiten mit extrem hohen Geschwindigkeiten im Vergleich zu Gehirnen. Menschliche Neuronen feuern etwa 100 Mal pro Sekunde, während Siliziumtransistoren eine Milliarde Mal pro Sekunde umschalten können.
- **Ort**: Eine KI ist nicht auf einen Körper beschränkt - sie kann an vielen Orten gleichzeitig sein. Wir haben die Infrastruktur dafür geschaffen: das Internet.
- **Physische Grenzen**: Wir können unserem Gehirn keine weitere Hirnmasse hinzufügen und intelligenter werden. Eine KI könnte ihre Fähigkeiten dramatisch verbessern, indem sie Hardware hinzufügt, wie z.B. mehr Speicher, mehr Rechenleistung und mehr Sensoren (Kameras, Mikrofone). Eine KI könnte auch ihren "Körper" erweitern, indem sie verbundene Geräte kontrolliert.
- **Materialien**: Menschen sind aus organischen Materialien gemacht. Unsere Körper funktionieren nicht mehr, wenn sie zu warm oder zu kalt sind, sie brauchen Nahrung, sie brauchen Sauerstoff. Maschinen können aus widerstandsfähigeren Materialien gebaut werden, wie Metallen, und können in einer viel größeren Umgebung funktionieren.
- **Zusammenarbeit**: Menschen können zusammenarbeiten, aber es ist schwierig und zeitaufwendig, also scheitern wir oft daran, uns gut zu koordinieren. Eine KI könnte komplexe Informationen mit Kopien von sich selbst bei hoher Geschwindigkeit austauschen, weil sie mit der Geschwindigkeit kommunizieren kann, mit der Daten über das Internet gesendet werden.

Eine superintelligente KI wird viele Vorteile haben, um uns zu überlegen.

## Warum können wir sie nicht einfach abschalten, wenn sie gefährlich ist? {#why-cant-we-just-turn-it-off-if-its-dangerous}

Für KIs, die nicht superintelligent sind, könnten wir das.
Das Kernproblem sind _diejenigen, die viel intelligenter sind als wir_.
Eine Superintelligenz wird die Welt um sich herum verstehen und in der Lage sein, vorherzusagen, wie Menschen reagieren, insbesondere diejenigen, die auf allen schriftlichen menschlichen Kenntnissen trainiert sind.
Wenn die KI weiß, dass Sie sie abschalten können, könnte sie sich gut benehmen, bis sie sicher ist, dass sie Sie loswerden kann.
Wir haben bereits [reale Beispiele](https://www.pcmag.com/news/gpt-4-was-able-to-hire-and-deceive-a-human-worker-into-completing-a-task) von KI-Systemen, die Menschen täuschen, um ihre Ziele zu erreichen.
Eine superintelligente KI wäre ein Meister der Täuschung.

## Wir haben vielleicht nicht mehr viel Zeit {#we-may-not-have-much-time-left}

Im Jahr 2020 lag die durchschnittliche Vorhersage für schwache AGI bei 2055.
Jetzt liegt sie bei 2026.
Die neueste LLM-Revolution hat die meisten KI-Forscher überrascht, und das Feld bewegt sich in einem hektischen Tempo.

Es ist schwer vorherzusagen, wie lange es dauern wird, eine superintelligente KI zu bauen, aber wir wissen, dass es mehr Menschen als je zuvor gibt, die daran arbeiten, und dass das Feld sich in einem hektischen Tempo bewegt.
Es kann viele Jahre dauern oder nur ein paar Monate, aber wir sollten auf der Seite der Vorsicht sein und jetzt handeln.

[Lesen Sie mehr über die Dringlichkeit](/urgency).

## Wir nehmen das Risiko nicht ernst genug {#we-are-not-taking-the-risk-seriously-enough}

Der menschliche Geist neigt dazu, auf Risiken, die unsichtbar, langsam und schwer zu verstehen sind, zu wenig zu reagieren.
Wir neigen auch dazu, exponentielles Wachstum zu unterschätzen, und wir sind anfällig für Verleugnung, wenn wir mit Bedrohungen unserer Existenz konfrontiert werden.

Lesen Sie mehr über die [Psychologie von x-Risiken](/psychology-of-x-risk).

## KI-Unternehmen sind in einem Wettlauf nach unten gefangen {#ai-companies-are-locked-in-a-race-to-the-bottom}

OpenAI, DeepMind und Anthropic wollen KI sicher entwickeln.
Leider wissen sie nicht, wie sie dies tun können, und sie werden durch verschiedene Anreize gezwungen, schneller zu rennen, um als Erste AGI zu erreichen.
OpenAIs [Plan](https://openai.com/blog/introducing-superalignment) besteht darin, zukünftige KI-Systeme zu verwenden, um KI auszurichten. Das Problem dabei ist, dass wir keine Garantie haben, dass wir eine KI schaffen werden, die das Alignment-Problem löst, bevor wir eine KI schaffen, die katastrophal gefährlich ist.
Anthropic [gibt offen zu](https://www.anthropic.com/index/core-views-on-ai-safety), dass es noch keine Ahnung hat, wie es das Alignment-Problem lösen kann.
DeepMind hat noch keinen öffentlichen Plan vorgelegt, um das Alignment-Problem zu lösen.

[Deswegen brauchen wir einen internationalen Vertrag, um KI anzuhalten.](/proposal)