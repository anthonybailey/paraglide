

---
title: Warum KI-Sicherheit wichtig ist
description: Bildungsressourcen (Videos, Artikel, Bücher) zu KI-Risiken und KI-Alignment
---
## Auf dieser Website {#on-this-website}

- [Risiken](/risks). Eine Zusammenfassung der Risiken von künstlicher Intelligenz.
- [X-Risiko](/xrisk). Warum KI ein existenzielles Risiko darstellt.
- [Übernahme](/ai-takeover). Wie KI die Welt übernehmen könnte.
- [Zitate](/quotes). Zitate zu KI-Risiken und -Governance.
- [Machbarkeit einer Pause](/feasibility). Die Machbarkeit einer Pause in der KI-Entwicklung.
- [Den Pausenknopf bauen](/building-the-pause-button). Was es braucht, um KI zu pausieren.
- [FAQ](/faq). Häufig gestellte Fragen zu KI-Sicherheit und PauseAI.
- [Aktion](/action). Was Sie tun können, um zu helfen (mit Links zu vielen aktionsbezogenen Anleitungen)

## Andere Websites {#other-websites}

- [Das Kompendium](https://www.thecompendium.ai/). Ein umfassendes Wissensbündel darüber, warum das aktuelle KI-Rennen so gefährlich ist und was wir dagegen tun können.
- [Ein schmaler Pfad](https://www.narrowpath.co/). Ein detaillierter Plan zu den Schritten, die wir unternehmen müssen, um unsere Chancen zu erhöhen, die nächsten Jahrzehnte zu überleben.
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). Die Landingpages für KI-Sicherheit. Erfahren Sie mehr über die Risiken, Gemeinschaften, Veranstaltungen, Jobs, Kurse, Ideen, wie man die Risiken mildern kann, und vieles mehr!
- [Existenzielle Sicherheit](https://existentialsafety.org/). Eine umfassende Liste von Maßnahmen, die wir ergreifen können, um unsere existenzielle Sicherheit vor KI zu erhöhen.
- [AISafety.dance](https://aisafety.dance). Eine unterhaltsamere, freundlichere und interaktivere Einführung in die KI-Katastrophenrisiken!
- [AISafety.world](https://aisafety.world/tiles/). Die gesamte KI-Sicherheitslandschaft mit allen Organisationen, Medien, Foren, Blogs und anderen Akteuren und Ressourcen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Datenbank von Vorfällen, bei denen KI-Systeme Schaden verursacht haben.

- [LethalIntelligence.ai](https://lethalintelligence.ai/). Eine Sammlung von Ressourcen zu KI-Risiken und KI-Alignment.

## Newsletter {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Unser Newsletter.
- [TransformerNews](https://www.transformernews.ai/) Umfassender wöchentlicher Newsletter zu KI-Sicherheit und -Governance.
- [Keine Sorge um die Vase](https://thezvi.substack.com/): Ein Newsletter über KI-Sicherheit, Rationalität und andere Themen.

## Videos {#videos}



- [PauseAI-Playlist](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) ist eine von uns zusammengestellte YouTube-Playlist mit Videos von 1 Minute bis 1 Stunde in verschiedenen Formaten und von diversen Quellen, die keine Vorkenntnisse erfordern.
- [Robert Miles' YouTube](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) ist ein großartiger Ort, um die Grundlagen des KI-Alignments zu verstehen.
- [LethalIntelligence's YouTube](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts {#podcasts}

- [DoomDebates](https://www.youtube.com/@DoomDebates) von Liron Shapira, komplett auf KI-Doom fokussiert.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) von Ex-Nachrichtensprecher John Sherman.
- [Future of Life Institute | Connor Leahy über KI-Sicherheit und warum die Welt fragil ist](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview mit Connor über die KI-Sicherheitsstrategien.
- [Lex Fridman | Max Tegmark: Der Fall für die Einstellung der KI-Entwicklung](https://youtu.be/VcVfceTsD0A?t=1547). Interview, das in die Details unserer aktuellen gefährlichen Situation eintaucht.
- [Sam Harris | Eliezer Yudkowsky: KI, auf dem Weg zum Abgrund](https://samharris.org/episode/SE60B0CF4B8). Gespräch über die Natur der Intelligenz, verschiedene Arten von KI, das Alignment-Problem, Ist vs. Soll und vieles mehr. Eine von vielen Episoden, die Making Sense über KI-Sicherheit hat.
- [Connor Leahy, KI-Feueralarm](https://youtu.be/pGjyiqJZPJo?t=2510). Vortrag über die Intelligenzexplosion und warum sie das wichtigste Ereignis wäre, das jemals passieren könnte.
- [Die 80.000 Stunden-Podcast empfohlene Episoden zu KI](https://80000hours.org/podcast/on-artificial-intelligence/). Nicht 80.000 Stunden lang, sondern eine Zusammenstellung von Episoden des 80.000 Stunden-Podcasts über KI-Sicherheit.
- [Future of Life Institute Podcast-Episoden zu KI](https://futureoflife.org/podcast/?_category_browser=ai). Alle Episoden des FLI-Podcasts über die Zukunft der künstlichen Intelligenz.

Podcasts mit PauseAI-Mitgliedern finden Sie in der [Medienberichterstattung](/press)-Liste.

## Artikel {#articles}

- [Das "Don't Look Up"-Denken, das uns mit KI zum Untergang bringen könnte](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (von Max Tegmark)
- [Das Pausieren der KI-Entwicklung reicht nicht aus. Wir müssen sie ganz abschalten](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (von Eliezer Yudkowsky)
- [Der Fall für das Verlangsamen von KI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (von Sigal Samuel)
- [Die KI-Revolution: Der Weg zur Superintelligenz](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (von WaitButWhy)
- [Wie sich böswillige KIs entwickeln könnten](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (von Yoshua Bengio)

- [Argumente gegen die Ernstnahme von KI-Sicherheit durchdenken](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (von Yoshua Bengio)
- Überprüfen Sie den Lesebereich auf [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

Wenn Sie lesen möchten, was Journalisten über PauseAI geschrieben haben, überprüfen Sie die Liste der [Medienberichterstattung](/press).

## Bücher {#books}



- [Unkontrollierbar: Die Bedrohung durch künstliche Superintelligenz und der Kampf, die Welt zu retten](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Holen Sie es sich kostenlos [hier](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [Der Abgrund: Existenzrisiko und die Zukunft der Menschheit](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Das Alignment-Problem](https://www.goodreads.com/book/show/50489349-the-alignment-problem) (Brian Christian, 2020)
- [Menschlich kompatibel: Künstliche Intelligenz und das Problem der Kontrolle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leben 3.0: Mensch sein im Zeitalter der künstlichen Intelligenz](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligenz: Wege, Gefahren, Strategien](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Unsere letzte Erfindung: Künstliche Intelligenz und das Ende der menschlichen Ära](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Papiere {#papers}

- [Eine Zusammenstellung](https://arkose.org/aisafety) von KI-Sicherheitspapieren
- [Eine weitere Zusammenstellung](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) von KI-Sicherheitspapieren
- [Alignment-Fälschung in großen Sprachmodellen](https://www.anthropic.com/news/alignment-faking) aktuelles Papier von Anthropic selbst
- [Management extremer KI-Risiken bei schnellem Fortschritt](https://www.science.org/doi/abs/10.1126/science.adn0117) von den Gründungsvätern des Fachgebiets

## Kurse {#courses}

- [AGI-Sicherheitsgrundlagen](https://www.agisafetyfundamentals.com/) (30 Stunden)
- [CHAI-Bibliographie empfohlener Materialien](https://humancompatible.ai/bibliography) (50 Stunden+)
- [AISafety.training](https://aisafety.training/): Überblick über Trainingsprogramme, Konferenzen und andere Veranstaltungen

## Organisationen {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startete den [offenen Brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleitet von Max Tegmark.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) ist ein Forschungszentrum an der Tschechischen Technischen Universität in Prag, geleitet von
- [Conjecture](https://www.conjecture.dev/). Start-up, das an KI-Alignment und KI-Politik arbeitet, geleitet von Connor Leahy.
- [Existenzrisiko-Observatorium](https://existentialriskobservatory.org/). Niederländische Organisation, die die Öffentlichkeit über X-Risiken informiert und Kommunikationsstrategien studiert.
- [Zentrum für die Regulierung von KI](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Zentrum für menschenkompatible künstliche Intelligenz](https://humancompatible.ai/about/) (CHAI), geleitet von Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), das mathematische Forschung zu KI-Sicherheit betreibt, geleitet von Eliezer Yudkowsky.
- [Institut für KI-Politik und -Strategie](https://www.iaps.ai/) (IAPS)
- [Das KI-Politik-Institut](https://theaipi.org/)
- [KI-Sicherheitskommunikationszentrum](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [Das Midas-Projekt](https://www.themidasproject.com/) Unternehmensdruckkampagnen für KI-Sicherheit.
- [Das Human Survival Project](https://thehumansurvivalproject.org/)
- [KI-Sicherheitswelt](https://aisafety.world/) Hier finden Sie eine Übersicht über die KI-Sicherheitslandschaft.

## Wenn Sie überzeugt sind und handeln möchten {#if-you-are-convinced-and-want-to-take-action}

Es gibt viele [Dinge, die Sie tun können](/action).
Ein Brief schreiben, an einer Demonstration teilnehmen, Geld spenden oder einer Gemeinschaft beitreten ist nicht so schwer!
Und diese Aktionen haben einen echten Einfluss.
Selbst wenn wir dem Ende der Welt gegenüberstehen, kann es noch Hoffnung und sehr lohnende Arbeit geben.

## Oder wenn Sie sich immer noch nicht ganz sicher sind {#or-if-you-still-dont-feel-quite-sure-of-it}

Das Lernen über die [Psychologie von X-Risiken](/psychology-of-x-risk) könnte Ihnen helfen.