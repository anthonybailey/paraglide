

---
title: Leer waarom AI-veiligheid belangrijk is
description: Educatieve bronnen (video's, artikelen, boeken) over AI-risico's en AI-alignering
---
## Op deze website {#on-this-website}

- [Risico's](/risks). Een overzicht van de risico's van AI.
- [X-risico](/xrisk). Waarom AI een existentieel risico vormt.
- [Overname](/ai-takeover). Hoe AI de wereld zou kunnen overnemen.
- [Citaten](/quotes). Citaten over AI-risico's en bestuur.
- [Haalbaarheid van een pauze](/feasibility). De haalbaarheid van een pauze in AI-ontwikkeling.
- [De pauzeknop bouwen](/building-the-pause-button). Wat nodig is om AI te pauzeren.
- [FAQ](/faq). Veelgestelde vragen over AI-veiligheid en PauseAI.
- [Actie](/action). Wat je kunt doen om te helpen (met links naar veel actiegerelateerde gidsen)

## Andere websites {#other-websites}

- [The Compendium](https://www.thecompendium.ai/). Een uitgebreide bundel kennis over waarom de huidige AI-race zo gevaarlijk is en wat we eraan kunnen doen.
- [A Narrow Path](https://www.narrowpath.co/). Een gedetailleerd plan over de stappen die we moeten nemen om onze kansen op overleven in de komende decennia te vergroten.
- [AISafety.com](https://www.aisafety.com) & [AISafety.info](https://aisafety.info). De landingspagina's voor AI-veiligheid. Leer over de risico's, gemeenschappen, evenementen, banen, cursussen, ideeën over hoe de risico's te verminderen en meer!
- [Existential Safety](https://existentialsafety.org/). Een uitgebreide lijst van acties die we kunnen nemen om onze existentiële veiligheid van AI te vergroten.
- [AISafety.dance](https://aisafety.dance). Een interactieve introductie tot de AI-catastrofische risico's!
- [AISafety.world](https://aisafety.world/tiles/). Het hele AI-veiligheidslandschap met alle organisaties, media-uitingen, forums, blogs en andere actoren en bronnen.
- [IncidentDatabase.ai](https://incidentdatabase.ai/). Database van incidenten waarbij AI-systemen schade hebben veroorzaakt.

- [LethalIntelligence.ai](https://lethalintelligence.ai/). Een verzameling bronnen over AI-risico's en AI-alignering.

## Nieuwsbrieven {#newsletters}

- [PauseAI Substack](https://pauseai.substack.com/): Onze nieuwsbrief.
- [TransformerNews](https://www.transformernews.ai/) Uitgebreide wekelijkse nieuwsbrief over AI-veiligheid en bestuur.
- [Don't Worry About The Vase](https://thezvi.substack.com/): Een nieuwsbrief over AI-veiligheid, rationaliteit en andere onderwerpen.

## Video's {#videos}

- [PauseAI Playlist](https://www.youtube.com/playlist?list=PLI46NoubGtIJa0JVCBR-9CayxCOmU0EJt) is een YouTube-afspeellijst die we hebben samengesteld, met video's van 1 minuut tot 1 uur in verschillende formaten en van diverse bronnen, en die geen voorkennis vereist.
- [Robert Miles' YouTube](https://www.youtube.com/watch?v=tlS5Y2vm02c&list=PLfHsskCxi_g-c62a_dmsNuHynaXsRQm40) zijn een geweldige plek om de fundamenten van AI-alignering te begrijpen.
- [LethalIntelligence's YouTube](https://www.youtube.com/channel/UCLwop3J1O7wL-PNWGjQw8fg)

## Podcasts {#podcasts}

- [DoomDebates](https://www.youtube.com/@DoomDebates) door Liron Shapira, volledig gericht op AI-doom.
- [For Humanity Podcast](https://www.youtube.com/@ForHumanityPodcast) door ex-nieuwsanker John Sherman.
- [Future of Life Institute | Connor Leahy over AI-veiligheid en waarom de wereld fragiel is](https://youtu.be/cSL3Zau1X8g?si=0X3EKoxZ80_HN9Rl&t=1803). Interview met Connor over de AI-veiligheidsstrategieën.
- [Lex Fridman | Max Tegmark: Het geval voor het stoppen van AI-ontwikkeling](https://youtu.be/VcVfceTsD0A?t=1547). Interview dat dieper ingaat op de details van onze huidige gevaarlijke situatie.
- [Sam Harris | Eliezer Yudkowsky: AI, racen naar de afgrond](https://samharris.org/episode/SE60B0CF4B8). Gesprek over de aard van intelligentie, verschillende soorten AI, het aligneringsprobleem, Is vs Ought, en meer. Een van de vele afleveringen van Making Sense over AI-veiligheid.
- [Connor Leahy, AI-brandalarm](https://youtu.be/pGjyiqJZPJo?t=2510). Praatje over de intelligentie-explosie en waarom het de belangrijkste gebeurtenis zou kunnen zijn die ooit plaatsvindt.
- [De 80.000 uur Podcast aanbevolen afleveringen over AI](https://80000hours.org/podcast/on-artificial-intelligence/). Een selectie van afleveringen van de 80.000 uur Podcast over AI-veiligheid.
- [Future of Life Institute Podcast-afleveringen over AI](https://futureoflife.org/podcast/?_category_browser=ai). Alle afleveringen van de FLI Podcast over de toekomst van kunstmatige intelligentie.

Podcasts met PauseAI-leden kunnen worden gevonden in de [media-aandacht](/press) lijst.

## Artikelen {#articles}

- [Het 'Don't Look Up'-denken dat ons met AI zou kunnen vernietigen](https://time.com/6273743/thinking-that-could-doom-us-with-ai/) (door Max Tegmark)
- [Het pauzeren van AI-ontwikkeling is niet genoeg. We moeten het helemaal stoppen](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (door Eliezer Yudkowsky)
- [Het geval voor het vertragen van AI](https://www.vox.com/the-highlight/23621198/artificial-intelligence-chatgpt-openai-existential-risk-china-ai-safety-technology) (door Sigal Samuel)
- [De AI-revolutie: De weg naar superintelligentie](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (door WaitButWhy)
- [Hoe rogue AI's kunnen ontstaan](https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/) (door Yoshua Bengio)

- [Redeneren door argumenten tegen het serieus nemen van AI-veiligheid](https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/) (door Yoshua Bengio)
- Bekijk de Leessectie op [LethalIntelligence.ai](https://lethalintelligence.ai/reading-time/)

Als je wilt lezen wat journalisten over PauseAI hebben geschreven, bekijk dan de lijst van [media-aandacht](/press).

## Boeken {#books}

- [Oncontroleerbaar: De dreiging van kunstmatige superintelligentie en de race om de wereld te redden](https://www.goodreads.com/book/show/202416160-uncontrollable) (Darren McKee, 2023). Download het gratis via [deze link](https://impactbooks.store/cart/47288196366640:1?discount=UNCON-P3SFRS)!
- [De afgrond: Existentieel risico en de toekomst van de mensheid](https://www.goodreads.com/en/book/show/50963653) (Toby Ord, 2020)
- [Het aligneringsprobleem](https://www.goodreads.com/book/show/50489349-the-alignment-probleem) (Brian Christian, 2020)
- [Menselijk compatibel: Kunstmatige intelligentie en het probleem van controle](https://www.goodreads.com/en/book/show/44767248) (Stuart Russell, 2019)
- [Leven 3.0: Mens zijn in het tijdperk van kunstmatige intelligentie](https://www.goodreads.com/en/book/show/34272565) (Max Tegmark, 2017)
- [Superintelligentie: Paden, gevaren, strategieën](https://www.goodreads.com/en/book/show/20527133) (Nick Bostrom, 2014)
- [Onze laatste uitvinding: Kunstmatige intelligentie en het einde van het menselijke tijdperk](https://www.goodreads.com/en/book/show/17286699) (James Barrat, 2013)

## Papers {#papers}

- [Een compilatie](https://arkose.org/aisafety) van AI-veiligheidspapers
- [Een andere compilatie](https://futureoflife.org/resource/introductory-resources-on-ai-risks/#toc-44245428-2) van AI-veiligheidspapers
- [Alignment faking in grote taalmodellen](https://www.anthropic.com/news/alignment-faking) recent paper door Anthropic zelf
- [Het beheersen van extreme AI-risico's te midden van snelle vooruitgang](https://www.science.org/doi/abs/10.1126/science.adn0117) van de peetvaders van het veld

## Cursussen {#courses}

- [AGI-veiligheid fundamenten](https://www.agisafetyfundamentals.com/) (30 uur)
- [CHAI-bibliografie van aanbevolen materialen](https://humancompatible.ai/bibliography) (50 uur+)
- [AISafety.training](https://aisafety.training/): Overzicht van trainingsprogramma's, conferenties en andere evenementen

## Organisaties {#organizations}

- [Future of Life Institute](https://futureoflife.org/cause-area/artificial-intelligence/) startte de [open brief](https://futureoflife.org/open-letter/pause-giant-ai-experiments/), geleid door Max Tegmark.
- [Center for AI Safety](https://www.safe.ai/) (CAIS) is een onderzoekscentrum aan de Tsjechische Technische Universiteit in Praag, geleid door
- [Conjecture](https://www.conjecture.dev/). Start-up die werkt aan AI-alignering en AI-beleid, geleid door Connor Leahy.
- [Existential Risk Observatory](https://existentialriskobservatory.org/). Nederlandse organisatie die het publiek informeert over x-risico's en communicatiestrategieën bestudeert.
- [Centre for the Governance of AI](https://www.governance.ai/)
- [FutureSociety](https://thefuturesociety.org/about-us/)
- [Center for Human-Compatible Artificial Intelligence](https://humancompatible.ai/about/) (CHAI), geleid door Stuart Russell.
- [Machine Intelligence Research Institute](https://intelligence.org/) (MIRI), doet wiskundig onderzoek naar AI-veiligheid, geleid door Eliezer Yudkowsky.
- [Institute for AI Policy and Strategy](https://www.iaps.ai/) (IAPS)
- [The AI Policy Institute](https://theaipi.org/)
- [AI Safety Communications Centre](https://aiscc.org/2023/11/01/yougov-poll-83-of-brits-demand-companies-prove-ai-systems-are-safe-before-release/)
- [The Midas Project](https://www.themidasproject.com/) Bedrijfscampagnes voor AI-veiligheid.
- [The Human Survival Project](https://thehumansurvivalproject.org/)
- [AI Safety World](https://aisafety.world/) Hier vind je een overzicht van het AI-veiligheidslandschap.

## Als je overtuigd bent en actie wilt ondernemen {#if-you-are-convinced-and-want-to-take-action}

Er zijn veel [dingen die je kunt doen](/action).
Een brief schrijven, naar een protest gaan, wat geld doneren of lid worden van een gemeenschap is niet zo moeilijk!
En deze acties hebben een echte impact.
Zelfs als we voor het einde van de wereld staan, kan er nog steeds hoop zijn en zeer lonend werk worden gedaan.

## Of als je nog steeds niet helemaal zeker bent {#or-if-you-still-dont-feel-quite-sure-of-it}

Leren over de [psychologie van x-risico](/psychology-of-x-risk) kan je helpen.